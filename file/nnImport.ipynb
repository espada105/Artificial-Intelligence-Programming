{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nnImport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available() #apple m1/m2의 mps가 사용가능하면\n",
    "\n",
    "    else \"cpu\"\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         device = \"cuda\"\n",
    "#     elif torch.backends.mps.is_available():\n",
    "#         device = \"mps\"\n",
    "#     else:\n",
    "#         device = \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module): #nnModule을 상속받아 pytorch 신경망모델생성\n",
    "    def __init__(self): \n",
    "        super().__init__() #부모클래스 생성자 호출\n",
    "        self.flatten = nn.Flatten() #입력이미지 1차원으로 평탄화 (Flatten()층은 2D이미지를 1D 벡터로 변환)\n",
    "        self.linear_relu_stack = nn.Sequential( #nn.Sequential()은 여러층을 순차적으로 쌓을때 사용\n",
    "            nn.Linear(28 * 28, 512),    # nn.Linear() 28 x 28크기의 이미지를 받아 512개의 출력 노드생성\n",
    "            nn.ReLU(), # 활성화 함수로 신경망의 비선형성을 추가해서 복잡한 패턴을 학습할 수 있도록 돕는다. ReLU는 입력이 양수일 경우에는 그대로 출력하고, 음수일 경우에는 0을 출력\n",
    "            nn.Linear(512, 512),# 512개입력을 받아 다시 512개의 출력으로 변환\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10), # 512개의 입력을 받아 다시 10개의 출력으로 변환\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0228, 0.6690, 0.8058, 0.4911, 0.6588, 0.6225, 0.9246, 0.5762,\n",
       "          0.7925, 0.3222, 0.8062, 0.8526, 0.4853, 0.2271, 0.9539, 0.6439,\n",
       "          0.0247, 0.0550, 0.8429, 0.7862, 0.6860, 0.2651, 0.1220, 0.2964,\n",
       "          0.6204, 0.2916, 0.9505, 0.4246],\n",
       "         [0.9108, 0.9258, 0.5849, 0.5882, 0.1736, 0.3016, 0.2808, 0.8464,\n",
       "          0.0616, 0.3477, 0.1374, 0.3285, 0.6806, 0.9513, 0.5344, 0.8922,\n",
       "          0.2697, 0.9956, 0.5833, 0.4988, 0.5098, 0.2670, 0.8972, 0.1031,\n",
       "          0.6674, 0.5793, 0.0792, 0.7824],\n",
       "         [0.5826, 0.7797, 0.5781, 0.2102, 0.8577, 0.7628, 0.7249, 0.9451,\n",
       "          0.8688, 0.4513, 0.5268, 0.1834, 0.7481, 0.9836, 0.7784, 0.7895,\n",
       "          0.0518, 0.6023, 0.8163, 0.5217, 0.5166, 0.3642, 0.5564, 0.8072,\n",
       "          0.6522, 0.5593, 0.7845, 0.7567],\n",
       "         [0.4936, 0.1202, 0.7715, 0.2122, 0.8008, 0.0243, 0.3622, 0.5854,\n",
       "          0.0191, 0.6571, 0.6343, 0.9238, 0.4229, 0.2430, 0.1817, 0.1597,\n",
       "          0.3539, 0.4099, 0.3871, 0.1171, 0.4104, 0.5855, 0.8674, 0.4087,\n",
       "          0.6490, 0.2600, 0.7836, 0.2332],\n",
       "         [0.7089, 0.8534, 0.0502, 0.2620, 0.0287, 0.4535, 0.0825, 0.7304,\n",
       "          0.8511, 0.8806, 0.1629, 0.3690, 0.0570, 0.3421, 0.4990, 0.1190,\n",
       "          0.7875, 0.6457, 0.4078, 0.8862, 0.2578, 0.1507, 0.9131, 0.6183,\n",
       "          0.1298, 0.4840, 0.2466, 0.7208],\n",
       "         [0.0718, 0.1056, 0.5145, 0.6697, 0.6433, 0.0447, 0.2612, 0.9128,\n",
       "          0.7654, 0.3887, 0.7049, 0.0574, 0.7410, 0.0290, 0.7706, 0.3162,\n",
       "          0.5441, 0.8597, 0.7405, 0.4880, 0.6699, 0.0516, 0.9377, 0.9166,\n",
       "          0.4690, 0.6015, 0.3387, 0.2862],\n",
       "         [0.6524, 0.4162, 0.1294, 0.5240, 0.9721, 0.5155, 0.8339, 0.9435,\n",
       "          0.9658, 0.4009, 0.7135, 0.6238, 0.7623, 0.6813, 0.7025, 0.3090,\n",
       "          0.8814, 0.8792, 0.6462, 0.8510, 0.7833, 0.1006, 0.4693, 0.3259,\n",
       "          0.3157, 0.4432, 0.4827, 0.0682],\n",
       "         [0.6420, 0.0899, 0.5626, 0.8488, 0.9581, 0.4131, 0.0379, 0.5820,\n",
       "          0.6393, 0.7387, 0.3048, 0.9686, 0.4301, 0.1152, 0.4138, 0.7344,\n",
       "          0.9762, 0.2877, 0.0149, 0.1695, 0.2580, 0.1268, 0.6074, 0.8598,\n",
       "          0.3573, 0.4940, 0.5654, 0.9126],\n",
       "         [0.3290, 0.4442, 0.1800, 0.7651, 0.0842, 0.8420, 0.8393, 0.5815,\n",
       "          0.8516, 0.1843, 0.1751, 0.5641, 0.7722, 0.7353, 0.3880, 0.6835,\n",
       "          0.6665, 0.3528, 0.8178, 0.0746, 0.8605, 0.3148, 0.4192, 0.9597,\n",
       "          0.8734, 0.8526, 0.0408, 0.7398],\n",
       "         [0.2858, 0.2834, 0.3845, 0.0340, 0.6359, 0.9657, 0.2556, 0.8207,\n",
       "          0.5171, 0.2110, 0.7122, 0.8564, 0.8465, 0.8739, 0.9785, 0.7669,\n",
       "          0.2796, 0.8345, 0.5162, 0.1298, 0.3097, 0.8186, 0.7404, 0.5365,\n",
       "          0.9733, 0.6001, 0.9411, 0.2405],\n",
       "         [0.3741, 0.9224, 0.3621, 0.2740, 0.5902, 0.4164, 0.1048, 0.4666,\n",
       "          0.0078, 0.2453, 0.0086, 0.6364, 0.5427, 0.9986, 0.5057, 0.2026,\n",
       "          0.1737, 0.4430, 0.9127, 0.5499, 0.3645, 0.3059, 0.5898, 0.7322,\n",
       "          0.7812, 0.6615, 0.1772, 0.1949],\n",
       "         [0.7916, 0.2113, 0.7765, 0.1074, 0.9185, 0.8316, 0.1706, 0.1351,\n",
       "          0.5391, 0.5041, 0.3007, 0.0987, 0.0707, 0.7814, 0.9939, 0.5751,\n",
       "          0.2810, 0.1844, 0.5862, 0.8617, 0.9815, 0.4913, 0.7214, 0.9604,\n",
       "          0.9879, 0.9986, 0.5356, 0.5366],\n",
       "         [0.3115, 0.5258, 0.1528, 0.9680, 0.3491, 0.0202, 0.2510, 0.7481,\n",
       "          0.1307, 0.8178, 0.8120, 0.0820, 0.4821, 0.9658, 0.1518, 0.0422,\n",
       "          0.4387, 0.6375, 0.5554, 0.6189, 0.1578, 0.2358, 0.1286, 0.4776,\n",
       "          0.0865, 0.5209, 0.2768, 0.5705],\n",
       "         [0.4524, 0.1174, 0.2610, 0.4715, 0.7867, 0.8617, 0.3548, 0.4810,\n",
       "          0.1485, 0.4420, 0.5214, 0.5092, 0.4724, 0.2466, 0.9452, 0.7762,\n",
       "          0.9889, 0.4037, 0.6247, 0.2973, 0.0354, 0.0979, 0.4092, 0.6171,\n",
       "          0.2733, 0.5368, 0.1378, 0.7329],\n",
       "         [0.0116, 0.5608, 0.7708, 0.2226, 0.5301, 0.3914, 0.4759, 0.0592,\n",
       "          0.6911, 0.5160, 0.0094, 0.3887, 0.6070, 0.2782, 0.9215, 0.8205,\n",
       "          0.6624, 0.6670, 0.3468, 0.0109, 0.9219, 0.9509, 0.1031, 0.7843,\n",
       "          0.2353, 0.4119, 0.2290, 0.4702],\n",
       "         [0.1845, 0.3784, 0.4398, 0.0427, 0.0803, 0.2347, 0.8550, 0.2545,\n",
       "          0.0180, 0.1378, 0.8077, 0.2046, 0.8261, 0.6163, 0.6839, 0.2538,\n",
       "          0.3688, 0.4506, 0.4703, 0.2547, 0.8057, 0.2730, 0.3361, 0.2674,\n",
       "          0.5948, 0.5614, 0.4734, 0.5174],\n",
       "         [0.3792, 0.5471, 0.1744, 0.6488, 0.8398, 0.1963, 0.9100, 0.3361,\n",
       "          0.4749, 0.3725, 0.8572, 0.2736, 0.5849, 0.0336, 0.7804, 0.9688,\n",
       "          0.7101, 0.4652, 0.5551, 0.7881, 0.6738, 0.6259, 0.4954, 0.6992,\n",
       "          0.0150, 0.4740, 0.2522, 0.5547],\n",
       "         [0.9355, 0.3642, 0.1359, 0.2319, 0.3601, 0.6963, 0.3490, 0.4953,\n",
       "          0.8205, 0.5055, 0.2884, 0.5007, 0.0444, 0.5057, 0.6014, 0.8309,\n",
       "          0.8145, 0.3067, 0.2498, 0.5734, 0.9477, 0.8452, 0.7806, 0.6631,\n",
       "          0.6331, 0.4838, 0.5096, 0.3570],\n",
       "         [0.7064, 0.3502, 0.9526, 0.4139, 0.7103, 0.2233, 0.8748, 0.4774,\n",
       "          0.4678, 0.6623, 0.1354, 0.9204, 0.6493, 0.4710, 0.1113, 0.6267,\n",
       "          0.0932, 0.9971, 0.5861, 0.2281, 0.3382, 0.8607, 0.8509, 0.4314,\n",
       "          0.6011, 0.6911, 0.0293, 0.3391],\n",
       "         [0.0120, 0.9161, 0.1578, 0.0963, 0.5041, 0.4361, 0.4847, 0.5725,\n",
       "          0.0527, 0.2263, 0.2350, 0.4623, 0.5087, 0.8172, 0.7148, 0.4721,\n",
       "          0.5864, 0.7073, 0.2754, 0.3921, 0.0084, 0.9095, 0.1460, 0.3962,\n",
       "          0.6338, 0.1512, 0.0468, 0.0614],\n",
       "         [0.5628, 0.7269, 0.4741, 0.2948, 0.0210, 0.2333, 0.9115, 0.6138,\n",
       "          0.5480, 0.5784, 0.0961, 0.6726, 0.0709, 0.4982, 0.3705, 0.5436,\n",
       "          0.7946, 0.0688, 0.4485, 0.3955, 0.5875, 0.1030, 0.2693, 0.8650,\n",
       "          0.6189, 0.4546, 0.4333, 0.2845],\n",
       "         [0.4719, 0.1795, 0.8486, 0.8054, 0.7883, 0.1403, 0.2934, 0.5878,\n",
       "          0.8586, 0.2801, 0.9103, 0.5617, 0.1017, 0.8625, 0.2050, 0.2758,\n",
       "          0.3779, 0.7677, 0.7851, 0.7781, 0.5810, 0.0949, 0.9233, 0.8321,\n",
       "          0.7490, 0.6377, 0.7174, 0.7788],\n",
       "         [0.5925, 0.2311, 0.9231, 0.3642, 0.3750, 0.2347, 0.0107, 0.5630,\n",
       "          0.2269, 0.2140, 0.9493, 0.8529, 0.3294, 0.5458, 0.6850, 0.3406,\n",
       "          0.7952, 0.7005, 0.0100, 0.7899, 0.6684, 0.8644, 0.1365, 0.1423,\n",
       "          0.3050, 0.7737, 0.8436, 0.3327],\n",
       "         [0.9917, 0.7729, 0.2414, 0.4274, 0.9154, 0.9147, 0.7563, 0.9678,\n",
       "          0.3322, 0.4459, 0.7872, 0.5784, 0.0108, 0.0812, 0.6446, 0.1589,\n",
       "          0.0736, 0.0867, 0.4241, 0.3235, 0.5602, 0.7872, 0.7385, 0.4856,\n",
       "          0.9879, 0.1543, 0.1569, 0.9738],\n",
       "         [0.2312, 0.6501, 0.3787, 0.2926, 0.1316, 0.1630, 0.0475, 0.5210,\n",
       "          0.1863, 0.6345, 0.9467, 0.4412, 0.5256, 0.0839, 0.1145, 0.2262,\n",
       "          0.3603, 0.4167, 0.9762, 0.7115, 0.1966, 0.0423, 0.1325, 0.7442,\n",
       "          0.9239, 0.3020, 0.2676, 0.4909],\n",
       "         [0.2242, 0.0171, 0.5177, 0.1832, 0.4660, 0.1037, 0.6377, 0.8923,\n",
       "          0.8341, 0.6899, 0.3377, 0.2737, 0.1282, 0.4250, 0.2791, 0.3072,\n",
       "          0.7464, 0.2220, 0.6654, 0.0746, 0.7628, 0.5475, 0.3267, 0.1474,\n",
       "          0.7590, 0.3443, 0.6064, 0.7197],\n",
       "         [0.4900, 0.6007, 0.6493, 0.1949, 0.0895, 0.2523, 0.7541, 0.1797,\n",
       "          0.3615, 0.0330, 0.4504, 0.3899, 0.1940, 0.5702, 0.8267, 0.3577,\n",
       "          0.9534, 0.4409, 0.4115, 0.6703, 0.4908, 0.9568, 0.2810, 0.9266,\n",
       "          0.4257, 0.6583, 0.9926, 0.7539],\n",
       "         [0.2175, 0.3123, 0.5462, 0.9630, 0.8867, 0.5697, 0.1559, 0.5284,\n",
       "          0.8613, 0.9077, 0.4643, 0.7604, 0.1501, 0.4396, 0.6056, 0.1860,\n",
       "          0.4817, 0.4169, 0.8053, 0.2236, 0.1569, 0.7649, 0.9790, 0.6049,\n",
       "          0.5666, 0.9340, 0.4167, 0.9722]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,28,28,device = device) #배치크기,이미지 높이, 이미지 너비 즉 3차원 텐서가 생성됨\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0098, -0.0113,  0.0303,  0.0089,  0.1280,  0.0408,  0.0147,  0.0134,\n",
       "         -0.0715,  0.0173]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device) #모델 인스턴스 생성\n",
    "logits = model(x) #모델에 입력 전달\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0991, 0.0970, 0.1011, 0.0990, 0.1115, 0.1022, 0.0996, 0.0994, 0.0913,\n",
       "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits) #Softmax는 모델이 출력한 각 클래스에 대한 점수를 확률 값으로 변환함\n",
    "                                        #dim은 차원지정으로 각 클래스에 대해 확률을 계산하겠다는 의미다. \n",
    "                                        # 이 때 10개의 클래스 중에서 확률이 가장 높은 클래스를 찾기위한 계산이 실행됨\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred_probab.argmax(1)  #argmax() 는 가 장 큰값을 가진 요소의 인덱스를 반환한다\n",
    "                                #확률이 가장 높은 클래스의 인덱스(예측된 클래스)를 반환함\n",
    "y_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 이미지를 인공신경망에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28) #3개의 batch size(=3개의 샘플) 28* 28 무작위 이미지 생성\n",
    "print(input_image.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()  # # Flatten 레이어 정의 (이미지를 평탄화하여 1차원 벡터로 변환)\n",
    "flat_image = flatten(input_image)   #3차원이미지를 1차원으로 평탄화( 평탄화를 하면 3, 784가됨)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features = 28*28, out_features = 20) # 완전 연결층 (28*28 크기의 입력을 20차원의 출력으로 변환)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0612,  0.4974, -0.2536, -0.3341,  0.2775, -0.2063,  0.2912,  0.3169,\n",
       "         0.0920,  0.3634,  0.5115,  0.2207, -0.5207,  0.2055,  0.6799,  0.4224,\n",
       "         0.0033,  0.5624, -0.6482, -0.1190], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.2457,  0.6823, -0.3665, -0.3054, -0.2574, -0.1055,  0.4251,  0.2112,\n",
      "         -0.1701,  0.2264, -0.0447, -0.2398, -0.2520,  0.6438,  0.4460,  0.4927,\n",
      "          0.1897,  0.2440, -0.6202, -0.1820],\n",
      "        [-0.0612,  0.4974, -0.2536, -0.3341,  0.2775, -0.2063,  0.2912,  0.3169,\n",
      "          0.0920,  0.3634,  0.5115,  0.2207, -0.5207,  0.2055,  0.6799,  0.4224,\n",
      "          0.0033,  0.5624, -0.6482, -0.1190],\n",
      "        [ 0.2330,  0.5569, -0.4399, -0.3052,  0.1686, -0.2412,  0.4890,  0.3278,\n",
      "         -0.0456,  0.1872, -0.0890,  0.1861, -0.2933,  0.4187,  0.5743,  0.0200,\n",
      "          0.6499,  0.0979, -0.4231, -0.1286]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.2457, 0.6823, 0.0000, 0.0000, 0.0000, 0.0000, 0.4251, 0.2112, 0.0000,\n",
      "         0.2264, 0.0000, 0.0000, 0.0000, 0.6438, 0.4460, 0.4927, 0.1897, 0.2440,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.4974, 0.0000, 0.0000, 0.2775, 0.0000, 0.2912, 0.3169, 0.0920,\n",
      "         0.3634, 0.5115, 0.2207, 0.0000, 0.2055, 0.6799, 0.4224, 0.0033, 0.5624,\n",
      "         0.0000, 0.0000],\n",
      "        [0.2330, 0.5569, 0.0000, 0.0000, 0.1686, 0.0000, 0.4890, 0.3278, 0.0000,\n",
      "         0.1872, 0.0000, 0.1861, 0.0000, 0.4187, 0.5743, 0.0200, 0.6499, 0.0979,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 이미지를 인공신경망에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3398,  0.0816,  0.1478, -0.0609,  0.0675,  0.0871, -0.0525,  0.2202,\n",
       "         -0.0064, -0.0505],\n",
       "        [-0.3494,  0.1336, -0.0115, -0.1035,  0.0405,  0.0984, -0.0946,  0.1502,\n",
       "         -0.2442,  0.0565],\n",
       "        [-0.3332, -0.0158,  0.1282,  0.0039,  0.2118,  0.0633, -0.0229,  0.1547,\n",
       "         -0.0295, -0.1377]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 층을 순차적으로 연결한 신경망 모듈을 정의.\n",
    "# 입력이 첫 번째 층을 지나면 두 번째 층으로, 그리고 그 다음 층으로 계속 전달\n",
    "seq_modules = nn.Sequential( # \n",
    "    flatten, # 입력 텐서를 평탄화\n",
    "    layer1, # 완전 연결층으로, 평탄화된 이미지 벡터를 입력 받아 20개의 특성을 출력\n",
    "    nn.ReLU(), #활성화 함수 ReLU를 적용하여 음수 값을 0으로 변환. 이를 통해 비선형성을 도입\n",
    "    nn.Linear(20, 10)   # 완전 연결층으로, 20개의 특성을 받아 10개의 출력을 생성 \n",
    "                        # 10개의 클래스에 대한 예측값(logits)을 생성하는 데 사용\n",
    ")\n",
    "\n",
    "logits = seq_modules(input_image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0698, 0.1064, 0.1137, 0.0923, 0.1049, 0.1070, 0.0930, 0.1222, 0.0974,\n",
       "         0.0932],\n",
       "        [0.0720, 0.1167, 0.1009, 0.0920, 0.1063, 0.1126, 0.0929, 0.1186, 0.0800,\n",
       "         0.1080],\n",
       "        [0.0707, 0.0972, 0.1122, 0.0991, 0.1220, 0.1052, 0.0965, 0.1152, 0.0959,\n",
       "         0.0860]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1) #PyTorch에서 제공하는 Softmax 함수를 정의\n",
    "pred_probab = softmax(logits) #각 클래스에 대한 확률 값을 반환\n",
    "pred_probab # 각 입력 샘플에 대해 10개의 클래스에 속할 확률 값을 가지는 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight, | size: torch.Size([512, 784]) | Values: tensor([[-0.0302,  0.0263,  0.0275,  ...,  0.0108, -0.0121, -0.0333],\n",
      "        [ 0.0172, -0.0078,  0.0148,  ...,  0.0051, -0.0244,  0.0104]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.0.bias, | size: torch.Size([512]) | Values: tensor([0.0060, 0.0277], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.2.weight, | size: torch.Size([512, 512]) | Values: tensor([[-0.0418, -0.0397,  0.0408,  ..., -0.0275, -0.0279, -0.0196],\n",
      "        [-0.0336, -0.0105,  0.0083,  ...,  0.0016, -0.0087, -0.0146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.2.bias, | size: torch.Size([512]) | Values: tensor([-0.0176,  0.0289], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.4.weight, | size: torch.Size([10, 512]) | Values: tensor([[-0.0342,  0.0218, -0.0017,  ..., -0.0090,  0.0240, -0.0405],\n",
      "        [-0.0399, -0.0105, -0.0156,  ..., -0.0225, -0.0282, -0.0267]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.4.bias, | size: torch.Size([10]) | Values: tensor([0.0419, 0.0054], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\") \n",
    "\n",
    "for name, param in model.named_parameters(): #모델의 모든 파라미터에 대한 정보를 가져옴\n",
    "    print(f\"Layer: {name}, | size: {param.size()} | Values: {param[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수가 있는 가장 간단한 인공신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) #입력\n",
    "y = torch.zeros(3) #타깃\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad = True) #랜덤하게 초기화된 가중치 행렬로, 크기는 (5, 3). 입력 x와 연산할 가중\n",
    "# requires_grad=True: 이 가중치가 학습 가능한 파라미터임을 명시하여 역전파를 통해 기울기가 계산되도록 함\n",
    "\n",
    "b = torch.randn(3, requires_grad=True) # 크기가 3인 랜덤한 편향 벡터를 생성\n",
    "# 애도 기울기가 계산되도록 함\n",
    "\n",
    "z = torch.matmul(x, w) + b \n",
    "#행렬 곱을 수행. 여기서 x는 크기 [5]의 벡터이고 w는 크기 [5, 3]의 가중치 행렬이므로, 이 둘을 곱하면 크기 [3]인 벡터가 됨\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "# 시그모이드 함수와 이진 교차 엔트로피 손실을 결합한 손실함수. \n",
    "# 로짓 z와 타깃 y를 사용해 손실 값을 계산하며, 이 값은 모델 예측의 정확도를 평가하는 데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2219,  0.8247,  0.2360],\n",
       "        [ 0.9419, -0.5091,  1.0514],\n",
       "        [-0.5949, -0.2719, -0.2771],\n",
       "        [ 1.1201, -1.0129,  1.8684],\n",
       "        [-0.9022, -2.8916, -1.4446]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6487, -0.3163,  0.1187], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4355, -4.1771,  1.5528], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7424, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z와 관련된 그래디언트 함수 = <AddBackward0 object at 0x000001F1AE1D7250>\n",
      "loss와 관련된 그래디언트 함수<BinaryCrossEntropyWithLogitsBackward0 object at 0x000001F1FDF055E0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"z와 관련된 그래디언트 함수 = {z.grad_fn}\")\n",
    "print(f\"loss와 관련된 그래디언트 함수{loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3229, 0.0050, 0.2751],\n",
      "        [0.3229, 0.0050, 0.2751],\n",
      "        [0.3229, 0.0050, 0.2751],\n",
      "        [0.3229, 0.0050, 0.2751],\n",
      "        [0.3229, 0.0050, 0.2751]])\n",
      "tensor([0.3229, 0.0050, 0.2751])\n"
     ]
    }
   ],
   "source": [
    "loss.backward() # 호출하면 그래디언트가 계산된다, 한 번만 가능, \n",
    "                # 역전파 수행으로 파라미터에 대한 기울기가 계산됨=> 오차를 줄이기위해 가중치를 얼마나 조정해야하는지 나옴\n",
    "print(w.grad) # 각 파라미터가 손실에 얼마나 영향을 미치는지\n",
    "print(b.grad) # 각 파라미터가 손실에 얼마나 영향을 미치는지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일부 그래디언트 계산이 필요없다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b # 선형변환을 수행\n",
    "print(z.requires_grad) #텐서가 기울기를 추적하고 있는지 여부 즉, 역전파를 통해 z에 대한 기울기를 계산할 수 있는지\n",
    "\n",
    "# with에서만 비활성화\n",
    "with torch.no_grad():# torch.no_grad() 블록 내에서는 기울기 계산이 비활성화(학습과 기울기 계산이 필요 없는 상황에서 사용)\n",
    "    z = torch.matmul(x, w) + b#\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b #torch.matmul(x, w): x와 w를 행렬 곱하여, 입력과 가중치의 선형 조합을 계산\n",
    "z_det = z.detach()  # z.detach()**는 텐서 z에서 기울기 추적을 분리하여 **새로운 텐서 z_det**를 만듦\n",
    "                    # z와 같은 값을 가지지만, 그래디언트를 추적하지 않는 텐서를 생성\n",
    "\n",
    "print(z_det.requires_grad) #기울기 추적 여부"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
